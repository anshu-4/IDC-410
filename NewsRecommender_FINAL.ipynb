{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWS RECOMMENDER\n",
    "\n",
    "\n",
    "Some of the outputs and visualizations used in the report might not be produced while running the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRAPING DATA\n",
    "\n",
    "\n",
    "\n",
    "Uncomment and run the below cell to scrape data from BBC.com. The csv file keeps appending after every run. Otherwise, load a downloaded data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# import re\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import csv\n",
    "\n",
    "# from newspaper import Article \n",
    "\n",
    "# url_base = \"https://bbc.com\"\n",
    "# sub_url = ['/news/','/sports/','/reel/','/worklife/','/travel/','/future/']\n",
    "# sub_url = [url_base + i for i in sub_url]\n",
    "\n",
    "# links = []\n",
    "\n",
    "# for url in sub_url:\n",
    "#     response = requests.get(url)\n",
    "#     data = response.text\n",
    "#     soup = BeautifulSoup(data,'lxml')\n",
    "#     tags = soup.find_all('a')\n",
    "#     for tag in tags:\n",
    "#         links.append(tag.get('href'))\n",
    "\n",
    "# new_list=[]\n",
    "# for i in links:\n",
    "#     if '/news/' in i:\n",
    "#         new_list.append(i)\n",
    "#     if '/sports/' in i:\n",
    "#         new_list.append(i)\n",
    "#     if '/reel/' in i:\n",
    "#         new_list.append(i)\n",
    "#     if '/worklife/' in i:\n",
    "#         new_list.append(i)\n",
    "#     if '/travel/' in i:\n",
    "#         new_list.append(i)\n",
    "#     if '/future/' in i:\n",
    "#         new_list.append(i)\n",
    "\n",
    "# newlist = []\n",
    "# for i in new_list:\n",
    "#     if i[0] == '/':\n",
    "#         continue\n",
    "#     else:\n",
    "#         newlist.append(i)\n",
    "#         new_list.remove(i)\n",
    "# newlist = [url_base + i for i in new_list]\n",
    "# for i  in newlist:\n",
    "#     if 'comhttps' in i:\n",
    "#         newlist.remove(i)\n",
    "\n",
    "# articles = []\n",
    "# for url in newlist:\n",
    "#     article = Article(url)\n",
    "#     try:    \n",
    "#         articles.append(article)\n",
    "#         article.download()\n",
    "#         article.parse()\n",
    "#         article.nlp()\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "# article_link = []\n",
    "# article_title = []\n",
    "# article_text = []\n",
    "# article_author = []\n",
    "# article_keywords = []\n",
    "# article_summary = []\n",
    "# for article in articles:\n",
    "#     article_link.append(article.url)\n",
    "#     article_author.append(article.authors)\n",
    "#     article_keywords.append(article.keywords)\n",
    "#     article_summary.append(article.summary)\n",
    "#     article_text.append(article.text)\n",
    "#     article_title.append(article.title)\n",
    "    \n",
    "# df['Title'] = article_title\n",
    "# df['Link'] = article_link\n",
    "# df['Authors'] = article_author\n",
    "# df['Article Text'] = article_text\n",
    "# df['Summary'] = article_summary\n",
    "# df['Keywords'] = article_keywords\n",
    "# df.to_csv('BBC_scraped.csv',mode=\"a\",header=False)\n",
    "# datafile = 'BBC_scraped.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to load an existing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "99df159d8549d11d99e8a349a4a7893812b187f2"
   },
   "outputs": [],
   "source": [
    "datafile = 'C:/Users/quiz4/Downloads/NewsArticles.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "raw_data = pd.read_csv(datafile, parse_dates=[0], infer_datetime_format=True,encoding='latin1')\n",
    "raw_data=raw_data.dropna(subset=['Article Text'])  \n",
    "reindexed_data = raw_data['Article Text']\n",
    "reindexed_data.index = raw_data['Link']\n",
    "article_length=[]\n",
    "for i in range(len(reindexed_data)):\n",
    "    article_length.append(len(reindexed_data[i].split()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data=raw_data[['Link','Title','Article Text']]\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning, Stemming, and Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expanding Stop Word list\n",
    "my_additional_stop_word_list = ['said','people','says','told','from']\n",
    "from sklearn.feature_extraction import text \n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_word_list)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "def sent_to_words(sentences): #This function removes unnecessary characters, stop words, and short words, and lowercases the text \n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  \n",
    "        sent = re.sub('\\s+', ' ', sent)  \n",
    "        sent = re.sub(\"\\'\", \"\", sent)\n",
    "        sent= re.sub('[^\\w_\\s-]',' ', sent)\n",
    "        sent = word_tokenize(sent)\n",
    "        sent = [w.lower() for w in sent if not w.lower() in stop_words]\n",
    "        sent = [i for i in sent if i.isalpha() and len(i)>2]   \n",
    "        yield(sent)  \n",
    "        \n",
    "        \n",
    "import nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "word_stemmer = PorterStemmer()\n",
    "\n",
    "data = clean_data['Article Text'].values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "for i in range(len(data_words)):\n",
    "    for j in range(len(data_words[i])):\n",
    "        data_words[i][j]= word_stemmer.stem(data_words[i][j])\n",
    "\n",
    "detokenized_doc = []\n",
    "for i in range(len(clean_data)):\n",
    "    t = ' '.join(data_words[i])\n",
    "    detokenized_doc.append(t)\n",
    "clean_data['cleaned_text']=detokenized_doc \n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexed_data= clean_data['cleaned_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1cdbefb7-9e8e-4a11-9cee-d39f8f16f557",
    "_uuid": "c31f278963d7bdd4c3553ada1fdb6095d50ca658"
   },
   "source": [
    "### Some Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "db6ce2f5-1247-4446-90f2-6aa2ba8af168",
    "_kg_hide-input": true,
    "_uuid": "59f4ae5e8e06786fa3ebdec7ec3011645aad3544"
   },
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def get_top_n_words(n_top_words, count_vectorizer, text_data):\n",
    "    '''\n",
    "    returns a tuple of the top n words in a sample and their \n",
    "    accompanying counts, given a CountVectorizer object and text sample\n",
    "    '''\n",
    "    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n",
    "    vectorized_total = np.sum(vectorized_headlines, axis=0)\n",
    "    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n",
    "    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n",
    "    \n",
    "    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n",
    "    for i in range(n_top_words):\n",
    "        word_vectors[i,word_indices[0,i]] = 1\n",
    "\n",
    "    words = [word[0].encode('ascii').decode('utf-8') for \n",
    "             word in count_vectorizer.inverse_transform(word_vectors)]\n",
    "\n",
    "    return (words, word_values[0,:n_top_words].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a2701525-298e-4943-8d3e-adad5d0d629d",
    "_uuid": "142e18023411d5c61cbe59a3c08c78c58993cdcc"
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "words, word_values = get_top_n_words(n_top_words=15,\n",
    "                                     count_vectorizer=count_vectorizer, \n",
    "                                     text_data=reindexed_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(range(len(words)), word_values);\n",
    "ax.set_xticks(range(len(words)));\n",
    "ax.set_xticklabels(words, rotation='vertical');\n",
    "ax.set_title('Top words in articles dataset (excluding stop words)');\n",
    "ax.set_xlabel('Word');\n",
    "ax.set_ylabel('Number of occurences');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "19b5a00b-e1c1-433a-aa4c-490bdd40b798",
    "_uuid": "0efcc663a7c6a267b0eba42378d902002677d422"
   },
   "source": [
    "### Feature Space Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "59988959-fa66-4c1f-9797-9a5ea3e6927d",
    "_uuid": "46fa661ac2dbb94d28fa1190172c52ddbce05c4a"
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=stop_words, max_features=40000)\n",
    "text_sample = reindexed_data\n",
    "\n",
    "document_term_matrix = count_vectorizer.fit_transform(text_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some required functions\n",
    "\n",
    "def get_keys(topic_matrix):\n",
    "    '''\n",
    "    returns an integer list of predicted topic \n",
    "    categories for a given topic matrix\n",
    "    '''\n",
    "    keys = topic_matrix.argmax(axis=1).tolist()\n",
    "    return keys\n",
    "\n",
    "def keys_to_counts(keys):\n",
    "    '''\n",
    "    returns a tuple of topic categories and their \n",
    "    accompanying magnitudes for a given list of keys\n",
    "    '''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)\n",
    "\n",
    "def get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n",
    "    '''\n",
    "    returns a list of n_topic strings, where each string contains the n most common \n",
    "    words in a predicted category, in order\n",
    "    '''\n",
    "    top_word_indices = []\n",
    "    for topic in range(n_topics):\n",
    "        temp_vector_sum = 0\n",
    "        #print(type(temp_vector_sum)) \n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                temp_vector_sum += document_term_matrix[i]\n",
    "                \n",
    "        #print(type(temp_vector_sum))     \n",
    "        temp_vector_sum = temp_vector_sum.toarray()\n",
    "        #print(type(temp_vector_sum))\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
    "        top_word_indices.append(top_n_word_indices)   \n",
    "    top_words = []\n",
    "    for topic in top_word_indices:\n",
    "        topic_words = []\n",
    "        for index in topic:\n",
    "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
    "            temp_word_vector[:,index] = 1\n",
    "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
    "            topic_words.append(the_word.encode('ascii','ignore').decode('utf-8'))\n",
    "        top_words.append(\" \".join(topic_words))         \n",
    "    return top_words\n",
    "\n",
    "\n",
    "def get_mean_topic_vectors(keys, two_dim_vectors):\n",
    "    '''\n",
    "    returns a list of centroid vectors from each predicted topic category\n",
    "    '''\n",
    "    mean_topic_vectors = []\n",
    "    for t in range(n_topics):\n",
    "        articles_in_that_topic = []\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == t:\n",
    "                articles_in_that_topic.append(two_dim_vectors[i])    \n",
    "        \n",
    "        articles_in_that_topic = np.vstack(articles_in_that_topic)\n",
    "        mean_article_in_that_topic = np.mean(articles_in_that_topic, axis=0)\n",
    "        mean_topic_vectors.append(mean_article_in_that_topic)\n",
    "    return mean_topic_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "297d53ca-9738-4bfe-825a-68d44af2fa1d",
    "_uuid": "8a5b831010134d1a88350a7a2257d6c15d19b9fe"
   },
   "source": [
    "### Latent Dirichilet Allocation (LDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c19bf4af-f399-4493-8fa5-d55e54e4f681",
    "_uuid": "0671476e00f484368beb747eb1529bfd680aa7fa"
   },
   "outputs": [],
   "source": [
    "n_topics=10\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online', \n",
    "                                          random_state=0, verbose=0)\n",
    "lda_topic_matrix = lda_model.fit_transform(document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "efef4bb1-e9c5-48db-bd16-1c08baeba814",
    "_uuid": "acec5dac6fac1dee4bf6e214c412a8435bcc6252"
   },
   "source": [
    "Once again, we take the $\\arg \\max$ of each entry in the topic matrix to obtain the predicted topic category for each headline. These topic categories can then be characterised by their most frequent words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "47111084-176f-449b-b756-b17c7404ce5c",
    "_uuid": "bb8244a14ca8c442449475aa0c9d484c15472d4a"
   },
   "outputs": [],
   "source": [
    "lda_keys = get_keys(lda_topic_matrix)\n",
    "lda_categories, lda_counts = keys_to_counts(lda_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9a43b1cb-9723-4a42-b13e-473eeb75c46b",
    "_uuid": "5f96953b1f371fa7a90b7fd22ff8af2e5249f15b"
   },
   "outputs": [],
   "source": [
    "top_n_words_lda = get_top_n_words(10, lda_keys, document_term_matrix, count_vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_lda)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_lda[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ecefeb7-f2eb-4cb5-aa2f-12fed8f2fdf6",
    "_uuid": "72ebe5380e7e18e169f5f5ebb1b54614b392cb12"
   },
   "source": [
    "The relative topic compositions of the sample are then illustated with a barchart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5842f40a-f0d5-496e-bfcd-6a2cc37b36ba",
    "_uuid": "0776b66c217ba13392ecb2fc0067e13a4f42c7ce"
   },
   "outputs": [],
   "source": [
    "top_3_words = get_top_n_words(3, lda_keys, document_term_matrix, count_vectorizer)\n",
    "labels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lda_categories]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(lda_categories, lda_counts);\n",
    "ax.set_xticks(lda_categories);\n",
    "ax.set_xticklabels(labels);\n",
    "ax.set_title('LDA topic counts');\n",
    "ax.set_ylabel('Number of articles');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting the data into two dimensions for visualization using $t$-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e4eb0aab-5559-414a-9355-db900af00334",
    "_uuid": "c2a0b485587e8efc66b3277f2b3a5e8271b6ee7c"
   },
   "outputs": [],
   "source": [
    "tsne_lda_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n",
    "                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\n",
    "tsne_lda_vectors = tsne_lda_model.fit_transform(lda_topic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\" ])\n",
    "colormap = colormap[:n_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "58efe6bb-5a22-4377-bc3d-9238472f1b6b",
    "_uuid": "ddd57511e2525587e7da1e3dd1bcbc5341c553c6"
   },
   "outputs": [],
   "source": [
    "top_3_words_lda = get_top_n_words(3, lda_keys, document_term_matrix, count_vectorizer)\n",
    "lda_mean_topic_vectors = get_mean_topic_vectors(lda_keys, tsne_lda_vectors)\n",
    "\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), plot_width=700, plot_height=700)\n",
    "plot.scatter(x=tsne_lda_vectors[:,0], y=tsne_lda_vectors[:,1], color=colormap[lda_keys])\n",
    "\n",
    "for t in range(n_topics):\n",
    "    label = Label(x=lda_mean_topic_vectors[t][0], y=lda_mean_topic_vectors[t][1], \n",
    "                  text=top_3_words_lda[t], text_color=colormap[t])\n",
    "    plot.add_layout(label)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "10fbd4b4-e66e-44f6-81b9-41c32bc61bc2",
    "_uuid": "ed65eb23ee8753bb741b26396d2451c9cc366c3d"
   },
   "outputs": [],
   "source": [
    "topics_docs=[]\n",
    "docs_percentage=[]\n",
    "documentid=[]\n",
    "for i in range(len(lda_topic_matrix)):\n",
    "    m= np.argmax(lda_topic_matrix[i])\n",
    "    topics_docs.append(m)\n",
    "    docs_percentage.append(lda_topic_matrix[i][m])\n",
    "    documentid.append(i)\n",
    "td=pd.DataFrame()\n",
    "td[\"Article ID\"]=documentid\n",
    "td[\"Topic\"]=topics_docs\n",
    "td[\"Percentage Contribution\"]=docs_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Dataframe (Topic-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the articles into lists according to the dominant topic of the article\n",
    "\n",
    "CORPUS=[]\n",
    "topic0=[]\n",
    "topic0per=[]\n",
    "doc0link=[]\n",
    "newlist=reindexed_data.index\n",
    "for i in range(len(newlist)):\n",
    "    if topics_docs[i]==0:\n",
    "        topic0.append(i)\n",
    "        topic0per.append(docs_percentage[i])\n",
    "        doc0link.append(newlist[i])\n",
    "t0=pd.DataFrame()\n",
    "t0['Document ID']=topic0\n",
    "t0['Percentage Contribution']=topic0per\n",
    "t0['Topic']=0\n",
    "t0[\"Link\"]=doc0link\n",
    "t0.sort_values(by=['Percentage Contribution'], inplace=True,ascending=False)\n",
    "CORPUS.append(t0)\n",
    "\n",
    "topic1=[]\n",
    "topic1per=[]\n",
    "doc1link=[]\n",
    "for i in range(len(newlist)):\n",
    "    if topics_docs[i]==1:\n",
    "        topic1.append(i)\n",
    "        topic1per.append(docs_percentage[i])\n",
    "        doc1link.append(newlist[i])\n",
    "t1=pd.DataFrame()\n",
    "t1['Document ID']=topic1\n",
    "t1['Percentage Contribution']=topic1per\n",
    "t1['Topic']=1\n",
    "t1[\"Link\"]=doc1link\n",
    "t1.sort_values(by=['Percentage Contribution'], inplace=True,ascending=False)\n",
    "CORPUS.append(t1)\n",
    "\n",
    "topic2=[]\n",
    "topic2per=[]\n",
    "doc2link=[]\n",
    "for i in range(len(newlist)):\n",
    "    if topics_docs[i]==2:\n",
    "        topic2.append(i)\n",
    "        topic2per.append(docs_percentage[i])\n",
    "        doc2link.append(newlist[i])\n",
    "t2=pd.DataFrame()\n",
    "t2['Document ID']=topic2\n",
    "t2['Percentage Contribution']=topic2per\n",
    "t2['Topic']=2\n",
    "t2[\"Link\"]=doc2link\n",
    "t2.sort_values(by=['Percentage Contribution'], inplace=True,ascending=False)\n",
    "CORPUS.append(t2)\n",
    "\n",
    "topic3=[]\n",
    "topic3per=[]\n",
    "doc3link=[]\n",
    "for i in range(len(newlist)):\n",
    "    if topics_docs[i]==3:\n",
    "        topic3.append(i)\n",
    "        topic3per.append(docs_percentage[i])\n",
    "        doc3link.append(newlist[i])\n",
    "t3=pd.DataFrame()\n",
    "t3['Document ID']=topic3\n",
    "t3['Percentage Contribution']=topic3per\n",
    "t3['Topic']=3\n",
    "t3[\"Link\"]=doc3link\n",
    "t3.sort_values(by=['Percentage Contribution'], inplace=True,ascending=False)\n",
    "CORPUS.append(t3)\n",
    "\n",
    "topic4=[]\n",
    "topic4per=[]\n",
    "doc4link=[]\n",
    "for i in range(len(newlist)):\n",
    "    if topics_docs[i]==4:\n",
    "        topic4.append(i)\n",
    "        topic4per.append(docs_percentage[i]) \n",
    "        doc4link.append(newlist[i])\n",
    "t4=pd.DataFrame()\n",
    "t4['Document ID']=topic4\n",
    "t4['Percentage Contribution']=topic4per\n",
    "t4['Topic']=4\n",
    "t4[\"Link\"]=doc4link\n",
    "t4.sort_values(by=['Percentage Contribution'], inplace=True,ascending=False)\n",
    "CORPUS.append(t4)\n",
    "\n",
    "topic5=[]\n",
    "topic5per=[]\n",
    "doc5link=[]\n",
    "for i in range(len(newlist)):\n",
    "    if topics_docs[i]==5:\n",
    "        topic5.append(i)\n",
    "        topic5per.append(docs_percentage[i])  \n",
    "        doc5link.append(newlist[i])\n",
    "t5=pd.DataFrame()\n",
    "t5['Document ID']=topic5\n",
    "t5['Percentage Contribution']=topic5per\n",
    "t5['Topic']=5\n",
    "t5[\"Link\"]=doc5link\n",
    "t5.sort_values(by=['Percentage Contribution'], inplace=True,ascending=False)\n",
    "CORPUS.append(t5)\n",
    "\n",
    "\n",
    "topic6=[]\n",
    "topic6per=[]\n",
    "doc6link=[]\n",
    "for i in range(len(newlist)):\n",
    "    if topics_docs[i]==6:\n",
    "        topic6.append(i)\n",
    "        topic6per.append(docs_percentage[i]) \n",
    "        doc6link.append(newlist[i])\n",
    "t6=pd.DataFrame()\n",
    "t6['Document ID']=topic6\n",
    "t6['Percentage Contribution']=topic6per\n",
    "t6['Topic']=6\n",
    "t6[\"Link\"]=doc6link\n",
    "t6.sort_values(by=['Percentage Contribution'], inplace=True,ascending=False)\n",
    "CORPUS.append(t6)\n",
    "\n",
    "topic7=[]\n",
    "topic7per=[]\n",
    "doc7link=[]\n",
    "for i in range(len(newlist)):\n",
    "    if topics_docs[i]==7:\n",
    "        topic7.append(i)\n",
    "        topic7per.append(docs_percentage[i])   \n",
    "        doc7link.append(newlist[i])\n",
    "t7=pd.DataFrame()\n",
    "t7['Document ID']=topic7\n",
    "t7['Percentage Contribution']=topic7per\n",
    "t7['Topic']=7\n",
    "t7[\"Link\"]=doc7link\n",
    "t7.sort_values(by=['Percentage Contribution'], inplace=True,ascending=False)\n",
    "CORPUS.append(t7)\n",
    "\n",
    "topic8=[]\n",
    "topic8per=[]\n",
    "doc8link=[]\n",
    "for i in range(len(newlist)):\n",
    "    if topics_docs[i]==8:\n",
    "        topic8.append(i)\n",
    "        topic8per.append(docs_percentage[i])  \n",
    "        doc8link.append(newlist[i])\n",
    "t8=pd.DataFrame()\n",
    "t8['Document ID']=topic8\n",
    "t8['Percentage Contribution']=topic8per\n",
    "t8['Topic']=8\n",
    "t8[\"Link\"]=doc8link\n",
    "t8.sort_values(by=['Percentage Contribution'], inplace=True,ascending=False)\n",
    "CORPUS.append(t8)\n",
    "\n",
    "topic9=[]\n",
    "topic9per=[]\n",
    "doc9link=[]\n",
    "for i in range(len(newlist)):\n",
    "    if topics_docs[i]==9:\n",
    "        topic9.append(i)\n",
    "        topic9per.append(docs_percentage[i])    \n",
    "        doc9link.append(newlist[i])\n",
    "t9=pd.DataFrame()\n",
    "t9['Document ID']=topic9\n",
    "t9['Percentage Contribution']=topic9per\n",
    "t9['Topic']=9\n",
    "t9[\"Link\"]=doc9link\n",
    "t9.sort_values(by=['Percentage Contribution'], inplace=True,ascending=False)\n",
    "CORPUS.append(t9)\n",
    "\n",
    "CORPUS[8] #example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling the time spent reading the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "avg_time=[]\n",
    "for i in range(len(article_length)):\n",
    "    avg_time.append(article_length[i]/250.0)\n",
    "    \n",
    "def article_time(article_index):\n",
    "    n = 1000\n",
    "    np.random.seed(0x5eed)\n",
    "    # Parameters of the mixture components\n",
    "    norm_params = np.array([[avg_time[article_index]/4, avg_time[article_index]/5],\n",
    "                            [avg_time[article_index],avg_time[article_index]/5]])\n",
    "    n_components = norm_params.shape[0]\n",
    "    # Weight of each component, in this case all of them are 1/2\n",
    "    weights = np.ones(n_components, dtype=np.float64) / float(n_components)\n",
    "    # A stream of indices from which to choose the component\n",
    "    mixture_idx = np.random.choice(n_components, size=n, replace=True, p=weights)\n",
    "    # y is the mixture sample\n",
    "    y = np.fromiter((ss.norm.rvs(*(norm_params[i])) for i in mixture_idx),\n",
    "                       dtype=np.float64)\n",
    "    return(random.choice(y))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we show a sample of the time allotment function created above. We're creating the sample points for article at index '1' and plotting them with the expected two component Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "np.random.seed(0x5eed)\n",
    "# Parameters of the mixture components\n",
    "norm_params = np.array([[avg_time[1]/4, avg_time[1]/5],\n",
    "                        [avg_time[1],avg_time[1]/5]])\n",
    "n_components = norm_params.shape[0]\n",
    "# Weight of each component, in this case all of them are 1/2\n",
    "weights = np.ones(n_components, dtype=np.float64) / float(n_components)\n",
    "# A stream of indices from which to choose the component\n",
    "mixture_idx = np.random.choice(n_components, size=n, replace=True, p=weights)\n",
    "# y is the mixture sample\n",
    "y = np.fromiter((ss.norm.rvs(*(norm_params[i])) for i in mixture_idx),\n",
    "                   dtype=np.float64)\n",
    "\n",
    "# Theoretical PDF plotting -- generate the x and y plotting positions\n",
    "xs = np.linspace(y.min(), y.max(), 200)\n",
    "ys = np.zeros_like(xs)\n",
    "\n",
    "for (l, s), w in zip(norm_params, weights):\n",
    "    ys += ss.norm.pdf(xs, loc=l, scale=s) * w\n",
    "\n",
    "plt.plot(xs, ys)\n",
    "plt.hist(y, density=True, bins=\"fd\")\n",
    "plt.xlabel(\"Time spent reading article\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLICKSTREAM GENERATION\n",
    "\n",
    "Proceed to generate sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLICKTHROUGH_PER_SESSION_REGULAR_USER=[] #number of articles read by an average user (old)\n",
    "TIME_SPENT_PER_SESSION_REGULAR_USER=[] #time spent by an average user (old)\n",
    "CLICKTHROUGH_PER_SESSION_NEW_USER=[] #number of articles read by an average user (new)\n",
    "TIME_SPENT_PER_SESSION_NEW_USER=[] #time spent by an average user (new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to get the profile of new users for the first session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_users=10 #ENTER THE NUMBER OF NEW USERS\n",
    "\n",
    "\n",
    "NEW_USER_PROFILE=[]\n",
    " \n",
    "for j in range(new_users):\n",
    "    user_id=[]\n",
    "    doc_given=[]\n",
    "    click=[]\n",
    "    time_per_article=[]\n",
    "    topic_id=[]\n",
    "    session_id=[]\n",
    "    \n",
    "    \n",
    "   \n",
    "    for i in range(10):\n",
    "        doc_given.append(random.choice(CORPUS[i]['Document ID']))\n",
    "        topic_id.append(i)\n",
    "        user_id.append(j)\n",
    "        session_id.append(1)\n",
    "        click.append(int(random.getrandbits(1)))\n",
    "        if click[i]==0:\n",
    "            time_per_article.append(0)\n",
    "        else:\n",
    "            a=1\n",
    "            while a==1:\n",
    "                t=article_time(doc_given[i])\n",
    "                if t>0 :\n",
    "                    a=0\n",
    "                    time_per_article.append(t)\n",
    "    \n",
    "    profile_new=pd.DataFrame()\n",
    "    profile_new['User ID']=user_id\n",
    "    profile_new[\"Session ID\"]=session_id\n",
    "    profile_new['Document ID']=doc_given\n",
    "    profile_new['Topic']=topic_id\n",
    "    profile_new['click']=click\n",
    "    profile_new['time_spent']=time_per_article\n",
    "\n",
    "    profile_new.sort_values(by=['User ID'], inplace=True,ascending=True)\n",
    "    NEW_USER_PROFILE.append(profile_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_USER_PROFILE[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to generate a session analysis for the new users for the first session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_new=pd.DataFrame()\n",
    "u_id=[]\n",
    "articles_read=[]\n",
    "time_spent=[]\n",
    "session_id=[]\n",
    "for i in range(new_users):\n",
    "    u_id.append(i)\n",
    "    session_id.append(1)\n",
    "    user_profile=NEW_USER_PROFILE[i]\n",
    "    time_spent.append(np.sum(user_profile['time_spent']))\n",
    "    articles_read.append(np.sum(user_profile['click']))\n",
    "session_new['Session ID']=session_id\n",
    "session_new['New User ID']=u_id\n",
    "session_new['Total time spent']=time_spent\n",
    "session_new['Articles read']=articles_read\n",
    "session_avg_time=np.sum(session_new['Total time spent']/new_users)\n",
    "print('Average time spent by a new user in this session',session_avg_time,\"\\n\")\n",
    "avg_clicks_per_user=np.sum(session_new[\"Articles read\"]/new_users)\n",
    "print(\"Average number of articles read by a new user  in this session \", avg_clicks_per_user,\"\\n\")\n",
    "category=[]\n",
    "for i in range(new_users):\n",
    "    if session_new.iloc[i]['Total time spent']<(session_avg_time*1.0/2):\n",
    "        category.append(\"Non regular\")\n",
    "    else:\n",
    "        category.append(\"Regular\")\n",
    "session_new['Expected category']=category\n",
    "print(session_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD USERS \n",
    "User profiling and session analysis for the regular users have been added in a similar manner as above and are contained in the session loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPDATING THE REGULAR USER REPOSITORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herw we add the new regular customers to our existing list of \"REGULAR_USERS\" and add the new session's data to our existing regular customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGULAR_USERS=[] #0 AT THE BEGINNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPDATING EXISTING LIST - RUNS FROM THE SECOND SESSION\n",
    "# for i in range(len(REGULAR_USERS)):\n",
    "#     REGULAR_USERS[i]=pd.concat([REGULAR_USERS[i],OLD_USER_PROFILE[i]])\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADDING NEW REGULAR USERS TO THE LIST\n",
    "n=len(REGULAR_USERS)\n",
    "print(\"Number pf regular users before this session\",len(REGULAR_USERS))\n",
    "for i in range(new_users):\n",
    "    if session_new.iloc[i]['Expected category']==\"Regular\":\n",
    "        REGULAR_USERS.append(NEW_USER_PROFILE[i])\n",
    "        REGULAR_USERS[n].loc[:,\"User ID\"]=n\n",
    "        n=n+1\n",
    "print(\"\\n Number of regular users after this session\",len(REGULAR_USERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USER VECTORS OF REGULAR USERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vector=[]\n",
    "for i in range(len(REGULAR_USERS)):\n",
    "    v=np.zeros(10)\n",
    "    user_vector.append(v)\n",
    "\n",
    "    user_profile=REGULAR_USERS[i].iloc[0:10]\n",
    "    total_time=np.sum(user_profile['time_spent'])\n",
    "    \n",
    "    for j in range(len(user_profile)):\n",
    "        user_vector[i]=user_vector[i]+((user_profile.iloc[j][\"time_spent\"]*1.0/total_time)*lda_topic_matrix[int(user_profile.iloc[j]['Document ID'])])\n",
    "        \n",
    "\n",
    "#USES SAME INDEX AS REGULAR_USERS\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "\t\"\"\"Takes 2 vectors a, b and returns the cosine similarity according \n",
    "\tto the definition of the dot product\n",
    "\t\"\"\"\n",
    "\tdot_product = np.dot(a, b)\n",
    "\tnorm_a = np.linalg.norm(a)\n",
    "\tnorm_b = np.linalg.norm(b)\n",
    "\treturn dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMILARITY COEFFICIENT FOR RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMILAR_DOCS=[]\n",
    "for j in range(len(user_vector)):\n",
    "    similarity=[]\n",
    "    docid=[]\n",
    "    topic_distri=[]\n",
    "    for i in range(len(lda_topic_matrix)):\n",
    "        docid.append(i)\n",
    "        cs= cos_sim(user_vector[j],lda_topic_matrix[i])\n",
    "        similarity.append(cs)\n",
    "        topic_distri.append(topics_docs[i])\n",
    "    #similarity.sort(reverse=True)\n",
    "    zipped = zip(docid, similarity)\n",
    "    similar_docs=pd.DataFrame()\n",
    "    similar_docs['Document ID']=docid\n",
    "    similar_docs['Similarity']=similarity\n",
    "    similar_docs['Links']=newlist\n",
    "    similar_docs[\"Topic\"]=topic_distri\n",
    "    similar_docs.sort_values(by=['Similarity'], inplace=True,ascending=False)\n",
    "    SIMILAR_DOCS.append(similar_docs)\n",
    "print(\"Top 10 similar documents:\")\n",
    "SIMILAR_DOCS[0].iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further sessions\n",
    "Run the following looped cell according to the number of sessions to execute.\n",
    "Please rerun the entire code if you wish to rerun the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_SESSIONS=2\n",
    "\n",
    "for m in range(NUMBER_OF_SESSIONS):\n",
    "    print (\"\\n SESSION\",m+2,\"\\n\")\n",
    "    new_users=10 #ENTER THE NUMBER OF NEW USERS\n",
    "\n",
    "\n",
    "    NEW_USER_PROFILE=[]\n",
    "\n",
    "    for j in range(new_users):\n",
    "        user_id=[]\n",
    "        doc_given=[]\n",
    "        click=[]\n",
    "        time_per_article=[]\n",
    "        topic_id=[]\n",
    "        session_id=[]\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(10):\n",
    "            doc_given.append(random.choice(CORPUS[i]['Document ID']))\n",
    "            topic_id.append(i)\n",
    "            user_id.append(j)\n",
    "            session_id.append(m+2)\n",
    "            click.append(int(random.getrandbits(1)))\n",
    "            if click[i]==0:\n",
    "                time_per_article.append(0)\n",
    "            else:\n",
    "                a=1\n",
    "                while a==1:\n",
    "                    t=article_time(doc_given[i])\n",
    "                    if t>0 :\n",
    "                        a=0\n",
    "                        time_per_article.append(t)\n",
    "\n",
    "        profile_new=pd.DataFrame()\n",
    "        profile_new['User ID']=user_id\n",
    "        profile_new['Session ID']=session_id\n",
    "        profile_new['Document ID']=doc_given\n",
    "        profile_new['Topic']=topic_id\n",
    "        profile_new['click']=click\n",
    "        profile_new['time_spent']=time_per_article\n",
    "\n",
    "        profile_new.sort_values(by=['User ID'], inplace=True,ascending=True)\n",
    "        NEW_USER_PROFILE.append(profile_new)\n",
    "\n",
    "    session_new=pd.DataFrame()\n",
    "    u_id=[]\n",
    "    articles_read=[]\n",
    "    time_spent=[]\n",
    "    session_id=[]\n",
    "    for i in range(new_users):\n",
    "        u_id.append(i)\n",
    "        session_id.append(m+2)\n",
    "        user_profile=NEW_USER_PROFILE[i]\n",
    "        time_spent.append(np.sum(user_profile['time_spent']))\n",
    "        articles_read.append(np.sum(user_profile['click']))\n",
    "    \n",
    "    session_new['New User ID']=u_id\n",
    "    session_new['Session ID']=session_id\n",
    "    session_new['Total time spent']=time_spent\n",
    "    session_new['Articles read']=articles_read\n",
    "    session_new_avg_time=np.sum(session_new['Total time spent']/new_users)\n",
    "    \n",
    "    print('\\n Average time spent by a new user in this session',session_new_avg_time,\"\\n\")\n",
    "    avg_clicks_per_new_user=np.sum(session_new[\"Articles read\"]/new_users)\n",
    "    print(\"\\n Average number of articles read by a new user  in this session \", avg_clicks_per_new_user,\"\\n\")\n",
    "    CLICKTHROUGH_PER_SESSION_NEW_USER.append(avg_clicks_per_new_user)\n",
    "    TIME_SPENT_PER_SESSION_NEW_USER.append(session_new_avg_time)\n",
    "    category=[]\n",
    "    for i in range(new_users):\n",
    "        if session_new.iloc[i]['Total time spent']<(session_avg_time*1.0/2):\n",
    "            category.append(\"Non regular\")\n",
    "        else:\n",
    "            category.append(\"Regular\")\n",
    "    session_new['Expected category']=category\n",
    "    print(session_new)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################_____________USER PROFILE FOR REGULAR USER__________########################################\n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    OLD_USER_PROFILE=[]\n",
    "\n",
    "\n",
    "    for j in range(len(REGULAR_USERS)): \n",
    "\n",
    "        user_id=[]\n",
    "        doc_given=[]\n",
    "        click=[]\n",
    "        time_per_article=[]\n",
    "        topic_id=[]\n",
    "        session_id=[]\n",
    "\n",
    "        ##RECOMMENDING ARTICLES\n",
    "        n=0\n",
    "        ind=0\n",
    "\n",
    "        while n<5:\n",
    "\n",
    "            if not(SIMILAR_DOCS[j].iloc[ind]['Document ID'] in REGULAR_USERS[j][\"Document ID\"]):\n",
    "                doc_given.append(SIMILAR_DOCS[j].iloc[ind]['Document ID'])\n",
    "                topic_id.append(SIMILAR_DOCS[j].iloc[ind]['Topic'])\n",
    "                ind=ind+1\n",
    "                n=n+1\n",
    "\n",
    "            else:\n",
    "                ind=ind+1\n",
    "\n",
    "        n=0\n",
    "        while n<5:\n",
    "\n",
    "            r=np.random.randint(0,len(td))\n",
    "            random_doc=td.iloc[r]['Article ID']\n",
    "            if not(random_doc in REGULAR_USERS[j][\"Document ID\"]) and not(random_doc in doc_given) :\n",
    "                doc_given.append(random_doc)\n",
    "                topic_id.append(td.iloc[r][\"Topic\"])\n",
    "                n=n+1\n",
    "        for i in range(10):\n",
    "            user_id.append(j)\n",
    "            session_id.append(m+2)\n",
    "            if SIMILAR_DOCS[j].iloc[int(doc_given[i])]['Similarity']>7:\n",
    "                a=int(random.getrandbits(1))\n",
    "                b=int(random.getrandbits(1))\n",
    "                if a==0 and b==0:\n",
    "                      click.append(0)\n",
    "                else:\n",
    "                      click.append(1)\n",
    "            else:\n",
    "                      click.append(int(random.getrandbits(1)))\n",
    "            if click[i]==0:\n",
    "                time_per_article.append(0)\n",
    "            else:\n",
    "                a=1\n",
    "                while a==1:\n",
    "                    t=article_time(int(doc_given[i]))\n",
    "                    if t>0 :\n",
    "                        a=0\n",
    "                        time_per_article.append(t)\n",
    "\n",
    "\n",
    "        profile_regular=pd.DataFrame()\n",
    "        profile_regular['User ID']=user_id\n",
    "        profile_regular[\"Session ID\"]=session_id\n",
    "        profile_regular['Document ID']=doc_given\n",
    "        profile_regular['Topic']=topic_id\n",
    "        profile_regular['click']=click\n",
    "        profile_regular['time_spent']=time_per_article\n",
    "\n",
    "        profile_regular.sort_values(by=['User ID'], inplace=True,ascending=True)\n",
    "        OLD_USER_PROFILE.append(profile_regular)\n",
    "        \n",
    "    \n",
    "    \n",
    "    #########################_____________SESSION ANALYSIS FOR OLD USERS_____________#####################\n",
    "    \n",
    "    \n",
    "    session_regular=pd.DataFrame()\n",
    "    u_id=[]\n",
    "    articles_read=[]\n",
    "    time_spent=[]\n",
    "    session_id=[]\n",
    "    for i in range(len(REGULAR_USERS)):\n",
    "        u_id.append(i)\n",
    "        session_id.append(m+2)\n",
    "        user_profile=OLD_USER_PROFILE[i]\n",
    "        time_spent.append(np.sum(user_profile['time_spent']))\n",
    "        articles_read.append(np.sum(user_profile['click']))\n",
    "    session_regular['User ID']=u_id\n",
    "    session_regular[\"Session ID\"]=session_id\n",
    "    session_regular['Total time spent']=time_spent\n",
    "    session_regular['Number of articles read']=articles_read\n",
    "    session_regular_avg_time=np.sum(session_regular['Total time spent']/len(REGULAR_USERS))\n",
    "    print('\\n Average time spent by a regular user in this session',session_regular_avg_time,\"\\n\")\n",
    "    avg_clicks_per_regular_user=np.sum(session_regular[\"Number of articles read\"]/len(REGULAR_USERS))\n",
    "    print(\"\\n Average number of articles read by a regular user  in this session \", avg_clicks_per_regular_user,\"\\n\")\n",
    "    print(session_regular)\n",
    "    \n",
    "    CLICKTHROUGH_PER_SESSION_REGULAR_USER.append(avg_clicks_per_regular_user)\n",
    "    TIME_SPENT_PER_SESSION_REGULAR_USER.append(session_regular_avg_time)\n",
    "    \n",
    "                                    \n",
    "                                    \n",
    "    ####################____________UPDATING REGULARY USER DATASET____________##########################                               \n",
    "                                    \n",
    "                                    \n",
    "    for i in range(len(REGULAR_USERS)):\n",
    "        REGULAR_USERS[i]=pd.concat([REGULAR_USERS[i],OLD_USER_PROFILE[i]])\n",
    "\n",
    "    n=len(REGULAR_USERS)\n",
    "    print(\"\\n Number of regular users before this session\",len(REGULAR_USERS),\"\\n\")\n",
    "    for i in range(new_users):\n",
    "        if session_new.iloc[i]['Expected category']==\"Regular\":\n",
    "            REGULAR_USERS.append(NEW_USER_PROFILE[i])\n",
    "            REGULAR_USERS[n].loc[:,\"User ID\"]=n\n",
    "            n=n+1\n",
    "    print(\"\\n Number of regular users after this session\",len(REGULAR_USERS),\"\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################________USER VECTOR____________##############################\n",
    "                                    \n",
    "                                    \n",
    "    user_vector=[]\n",
    "    for i in range(len(REGULAR_USERS)):\n",
    "        v=np.zeros(10)\n",
    "        user_vector.append(v)\n",
    "\n",
    "        user_profile=REGULAR_USERS[i].iloc[0:10]\n",
    "        total_time=np.sum(user_profile['time_spent'])\n",
    "\n",
    "        for j in range(len(user_profile)):\n",
    "            user_vector[i]=user_vector[i]+((user_profile.iloc[j][\"time_spent\"]*1.0/total_time)*lda_topic_matrix[int(user_profile.iloc[j]['Document ID'])])\n",
    "\n",
    "\n",
    "    ###################################_____SIMILARITY COEFFICIENT________###########################\n",
    "    \n",
    "    \n",
    "    SIMILAR_DOCS=[]\n",
    "    for j in range(len(user_vector)):\n",
    "        similarity=[]\n",
    "        docid=[]\n",
    "        topic_distri=[]\n",
    "        for i in range(len(lda_topic_matrix)):\n",
    "            docid.append(i)\n",
    "            cs= cos_sim(user_vector[j],lda_topic_matrix[i])\n",
    "            similarity.append(cs)\n",
    "            topic_distri.append(topics_docs[i])\n",
    "        #similarity.sort(reverse=True)\n",
    "        zipped = zip(docid, similarity)\n",
    "        similar_docs=pd.DataFrame()\n",
    "        similar_docs['Document ID']=docid\n",
    "        similar_docs['Similarity']=similarity\n",
    "        similar_docs['Links']=newlist\n",
    "        similar_docs[\"Topic\"]=topic_distri\n",
    "        similar_docs.sort_values(by=['Similarity'], inplace=True,ascending=False)\n",
    "        SIMILAR_DOCS.append(similar_docs)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
